{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import json\n",
    "from datasets import Dataset, DatasetDict\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the dataset\n",
    "f = open(r\"C:\\Users\\Leandro\\Desktop\\WandB_Projects\\LLM_alpaca\\alpaca_data_cleaned.json\")\n",
    "data = json.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define the number range (0 to 49999)\n",
    "rango_numeros = np.arange(len(data))\n",
    "\n",
    "# Generate valid_list with 1000 random numbers without repetition\n",
    "lista_valid = np.random.choice(rango_numeros, size=1000, replace=False)\n",
    "\n",
    "# Generate train_list with the remaining 49000 numbers\n",
    "lista_train = np.setdiff1d(rango_numeros, lista_valid)\n",
    "\n",
    "data_train = np.array(data)[lista_train]\n",
    "data_valid = np.array(data)[lista_valid]\n",
    "\n",
    "# Convert dictionary list to list dictionary\n",
    "data_dict_train = {key: [item[key] for item in data_train] for key in data_train[0]}\n",
    "data_dict_valid = {key: [item[key] for item in data_valid] for key in data_valid[0]}\n",
    "\n",
    "# Create a Dataset object from the list of dictionaries\n",
    "custom_dataset_train = Dataset.from_dict(data_dict_train)\n",
    "custom_dataset_valid = Dataset.from_dict(data_dict_valid)\n",
    "\n",
    "# Convert the Dataset to a DatasetDict\n",
    "dataset_dict = DatasetDict({\n",
    "    'train': custom_dataset_train,\n",
    "    'valid': custom_dataset_valid})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set train and valid datasets\n",
    "train_dataset = dataset_dict['train']\n",
    "eval_dataset = dataset_dict['valid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the prompts\n",
    "def prompt_no_input(row):\n",
    "    return (\"Below is an instruction that describes a task. \"\n",
    "            \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "            \"### Instruction:\\n{instruction}\\n\\n### Response:\\n{output}\").format_map(row)\n",
    "\n",
    "def prompt_input(row):\n",
    "    return (\"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
    "            \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "            \"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\\n{output}\").format_map(row)\n",
    "\n",
    "def create_prompt(row):\n",
    "    return prompt_no_input(row) if row[\"input\"] == \"\" else prompt_input(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the models to use\n",
    "\n",
    "model_id = 'openlm-research/open_llama_3b_v2'\n",
    "#model_id = 'bigscience/bloom-3b'\n",
    "#model_id = 'tiiuae/falcon-rw-1b'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=64,  # the rank of the LoRA matrices\n",
    "    lora_alpha=16, # the weight\n",
    "    lora_dropout=0.1, # dropout to add to the LoRA layers\n",
    "    bias=\"none\", # add bias to the nn.Linear layers?\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\", \"k_proj\",\"v_proj\",\"o_proj\"], # the name of the layers to add LoRA (LLAMA 3B)\n",
    "    #target_modules=['query_key_value'], # the name of the layers to add LoRA (BLOOM 3B)\n",
    "    #target_modules=['query_key_value'], # the name of the layers to add LoRA (Falcon 1B)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "# Quantization config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Is GPU available?\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "model_kwargs = dict(\n",
    "    device_map=device,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    use_cache=False,\n",
    "    quantization_config=bnb_config,\n",
    "    # low_cpu_mem_usage=True,\n",
    "    #use_flash_attention_2=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "batch_size = 1\n",
    "gradient_accumulation_steps = 16\n",
    "num_train_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "# Training parameters\n",
    "output_dir = \"./output/falcon-1b\"\n",
    "training_args = TrainingArguments(\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    fp16=True,\n",
    "    learning_rate=2e-4,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.1,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    #gradient_checkpointing=True,\n",
    "    gradient_checkpointing_kwargs=dict(use_reentrant=False),\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=1,\n",
    "    save_strategy=\"epoch\",\n",
    "    report_to=\"wandb\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "# Training parameters\n",
    "trainer = SFTTrainer(\n",
    "    model=model_id,\n",
    "    model_init_kwargs=model_kwargs,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    packing=True,\n",
    "    max_seq_length=1024,\n",
    "    args=training_args,\n",
    "    formatting_func=create_prompt,\n",
    "    peft_config=peft_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove answers\n",
    "def create_prompt_no_anwer(row):\n",
    "    row[\"output\"] = \"\"\n",
    "    return {\"text\": create_prompt(row)}\n",
    "\n",
    "test_dataset = eval_dataset.map(create_prompt_no_anwer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import LLMSampleCB\n",
    "\n",
    "# Set the wandb callback\n",
    "wandb_callback = LLMSampleCB(trainer, test_dataset, num_samples=30, max_new_tokens=256)\n",
    "trainer.add_callback(wandb_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training!\n",
    "with wandb.init(project='LLM_alpaca', name='falcon-1b'):\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INFERENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def limit_string(text, long_min=70, long_max=90):\n",
    "    full_text = []\n",
    "    chunks = text.split(\"\\n\")\n",
    "    chunks = [chunk for chunk in chunks if chunk != \"\"]\n",
    "\n",
    "    for chunk in chunks:\n",
    "        i = long_min\n",
    "        row = 1\n",
    "\n",
    "        while i < len(chunk):\n",
    "            if i < long_max * row and chunk[i] == ' ':\n",
    "                # Inserta un carácter de nueva línea en esa posición\n",
    "                chunk = chunk[:i] + '\\n' + chunk[i+1:]\n",
    "                i += long_min  # Salta al próximo bloque de caracteres\n",
    "                row += 1\n",
    "            else:\n",
    "                i += 1\n",
    "\n",
    "        full_text.append(chunk + \"\\n\\n\")\n",
    "\n",
    "    return \"\".join(full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saved_model = \"checkpoint-ynqkunya:v1\" # llama 3b\n",
    "#saved_model = \"checkpoint-mf8m86ws:v0\" # bloom 3b\n",
    "saved_model = \"checkpoint-n8itsr3a:v0\" # falcon 1b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Leandro\\Desktop\\WandB_Projects\\LLM_alpaca\\wandb\\run-20240416_130432-sqmxr3f3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/leandro-bello/LLM_alpaca/runs/sqmxr3f3' target=\"_blank\">royal-dawn-51</a></strong> to <a href='https://wandb.ai/leandro-bello/LLM_alpaca' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/leandro-bello/LLM_alpaca' target=\"_blank\">https://wandb.ai/leandro-bello/LLM_alpaca</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/leandro-bello/LLM_alpaca/runs/sqmxr3f3' target=\"_blank\">https://wandb.ai/leandro-bello/LLM_alpaca/runs/sqmxr3f3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact checkpoint-n8itsr3a:v0, 147.35MB. 13 files... \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   13 of 13 files downloaded.  \n",
      "Done. 0:0:0.7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0aec064a98a94a27ba59512528f83a65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">royal-dawn-51</strong> at: <a href='https://wandb.ai/leandro-bello/LLM_alpaca/runs/sqmxr3f3' target=\"_blank\">https://wandb.ai/leandro-bello/LLM_alpaca/runs/sqmxr3f3</a><br/> View project at: <a href='https://wandb.ai/leandro-bello/LLM_alpaca' target=\"_blank\">https://wandb.ai/leandro-bello/LLM_alpaca</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240416_130432-sqmxr3f3\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with wandb.init(project='LLM_alpaca'):\n",
    "    artifact = wandb.use_artifact(saved_model)\n",
    "    artifact_dir = artifact.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "trained_model = AutoModelForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path = artifact_dir,\n",
    "    return_dict = True,\n",
    "    quantization_config = bnb_config,\n",
    "    #trust_remote_code = True,\n",
    "    device_map = device)\n",
    "\n",
    "from peft import PeftModel\n",
    "\n",
    "trained_model = PeftModel.from_pretrained(\n",
    "    model = trained_model,\n",
    "    model_id = artifact_dir)\n",
    "\n",
    "trained_model_tokenizer = AutoTokenizer.from_pretrained(artifact_dir)\n",
    "trained_model_tokenizer.pad_token = trained_model_tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = trained_model.generation_config\n",
    "generation_config.max_new_token = 1024 * 2\n",
    "#generation_config.num_beams = 1\n",
    "#generation_config.early_stopping = True\n",
    "#generation_config.repetition_penalty = 0.5\n",
    "generation_config.temperature = 0.7\n",
    "#generation_config.top_p = 0.7\n",
    "#generation_config.top_k = 50\n",
    "generation_config.do_sample = True\n",
    "generation_config.num_return_sequence = 1\n",
    "generation_config.pad_token_id = trained_model_tokenizer.pad_token_id\n",
    "generation_config.eos_token_id = trained_model_tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    'What is the capital city of Australia?',\n",
    "    'Can you explain the theory of relativity in simple terms?',\n",
    "    'How would you code a function in Python to sum two vectors?',\n",
    "    'Who wrote the novel \"To Kill a Mockingbird\"?',\n",
    "    'What is the chemical formula for table salt?',\n",
    "    'How would you calculate the area of a circle given its radius?',\n",
    "    'Who was the first person to walk on the moon?',\n",
    "    'Can you name three common cybersecurity threats?',\n",
    "    'What is the meaning of the phrase \"a bird in the hand is worth two in the bush\"?',\n",
    "    'How would you explain the concept of sustainable development?',\n",
    "    'Who painted the famous artwork \"The Starry Night\"?',\n",
    "    'Can you describe the process of photosynthesis in plants?',\n",
    "    'How would you solve the following equation: 3x + 5 = 17?',\n",
    "    'How would you explain the concept of machine learning to someone with no technical background?',\n",
    "    'What do you think about the future of the artificial intelligence?'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "falcon_answers = []\n",
    "for q in questions:\n",
    "    instruction = q\n",
    "    print(\"QUESTION:\\n\" + instruction + \"\\n\")\n",
    "\n",
    "    prompt = f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "    ### Instruction:\n",
    "    {instruction}\n",
    "\n",
    "    ### Response:\n",
    "    \"\"\"\n",
    "\n",
    "    encoding = trained_model_tokenizer(\n",
    "        prompt,\n",
    "        padding = True,\n",
    "        truncation = True,\n",
    "        max_length = 1024 * 2,\n",
    "        return_tensors = 'pt').to(device)\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        outputs = trained_model.generate(\n",
    "            input_ids = encoding.input_ids,\n",
    "            attention_mask = encoding.attention_mask,\n",
    "            generation_config = generation_config,\n",
    "            max_new_tokens = 100 * 2\n",
    "        )\n",
    "\n",
    "    outputs = trained_model_tokenizer.decode(outputs[0], skip_special_tokens = True)\n",
    "\n",
    "    key = '### Response:'\n",
    "    pos = outputs.find(key) + len(key)\n",
    "    outputs_ = outputs[pos:]\n",
    "    print_outputs = limit_string(outputs_, long_min=70, long_max=90)\n",
    "    print(\"ANSWER:\\n\" + print_outputs)\n",
    "    falcon_answers.append(outputs_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with wandb.init(project='LLM_alpaca', name='log_qanda'):\n",
    "    data = []\n",
    "    for i, question in enumerate(questions, start=1):\n",
    "        data.append({\"id\": i, \n",
    "                     \"question\": question, \n",
    "                     \"llama-3b\": llama_answers_[i-1],\n",
    "                     \"bloom-3b\": bloom_answers_[i-1],\n",
    "                     \"falcon-1b\": falcon_answers_[i-1]})\n",
    "\n",
    "    data = pd.DataFrame(data)\n",
    "    table = wandb.Table(data=data, columns=[\"id\", \"question\", \"llama-3b\", \"bloom-3b\", \"falcon-1b\"])\n",
    "    wandb.log({\"questions\": table})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PERPLEXITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c827ca1007ea4b01816c1400f2f922ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 2164.0\n"
     ]
    }
   ],
   "source": [
    "# Compute the negative log-likelihoods\n",
    "nlls = []\n",
    "for i in tqdm(range(len(test_dataset))):\n",
    "\n",
    "    prompt = test_dataset[\"text\"][i]\n",
    "\n",
    "    # Prepare the test data\n",
    "    encoding = trained_model_tokenizer(\n",
    "        prompt,\n",
    "        padding = True,\n",
    "        truncation = True,\n",
    "        max_length = 1024 * 2,\n",
    "        return_tensors = 'pt').to(device)\n",
    "\n",
    "    input_ids = encoding[\"input_ids\"].to(device)\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        output = trained_model(input_ids, labels=input_ids)\n",
    "    nll = output.loss\n",
    "    nlls.append(nll)\n",
    "\n",
    "# Calculate the perplexity\n",
    "perplexity = torch.exp(torch.stack(nlls).mean())\n",
    "print(f\"Perplexity: {perplexity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLAMA\n",
    "# Perplexity: 2.082414150238037\n",
    "\n",
    "# BLOOM\n",
    "# Perplexity: 2.494140625\n",
    "\n",
    "# FALCON\n",
    "# Perplexity: 2164.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Leandro\\Desktop\\WandB_Projects\\LLM_alpaca\\wandb\\run-20240416_132545-4xlicqj3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/leandro-bello/LLM_alpaca/runs/4xlicqj3' target=\"_blank\">log_img</a></strong> to <a href='https://wandb.ai/leandro-bello/LLM_alpaca' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/leandro-bello/LLM_alpaca' target=\"_blank\">https://wandb.ai/leandro-bello/LLM_alpaca</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/leandro-bello/LLM_alpaca/runs/4xlicqj3' target=\"_blank\">https://wandb.ai/leandro-bello/LLM_alpaca/runs/4xlicqj3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0aba5e897cf4ecf97c1ef3301b8099e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.690 MB uploaded\\r'), FloatProgress(value=0.0019756179163244935, max=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">log_img</strong> at: <a href='https://wandb.ai/leandro-bello/LLM_alpaca/runs/4xlicqj3' target=\"_blank\">https://wandb.ai/leandro-bello/LLM_alpaca/runs/4xlicqj3</a><br/> View project at: <a href='https://wandb.ai/leandro-bello/LLM_alpaca' target=\"_blank\">https://wandb.ai/leandro-bello/LLM_alpaca</a><br/>Synced 4 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240416_132545-4xlicqj3\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "with wandb.init(project='LLM_alpaca', name='log_img'):\n",
    "\n",
    "    # Cargar tu imagen (reemplaza 'path_a_tu_imagen.jpg' con la ruta a tu imagen)\n",
    "    image_path = r\"C:\\Users\\Leandro\\Desktop\\Some projects\\LLM\\imgs\\parse_analysis.png\"\n",
    "    image = Image.open(image_path)\n",
    "\n",
    "    # Subir y registrar la imagen\n",
    "    wandb.log({\"image\": wandb.Image(image)})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TorchEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
